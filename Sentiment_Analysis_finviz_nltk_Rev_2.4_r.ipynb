{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "Some things you must know before using this app.:\n",
    "\n",
    "    -This app helps survey the most recent 100 posts or titles that appear on the finviz.com website for a particular stock symbol. Much thanks to the Python Project for sharing their code in how to scrape the website and the use of nltk's Vader sentiment analyzer to provide the sentiment ratings for each of the titles. The visual summary in the form of a bar chart is also great. The link to the Youtube video is provided below.\n",
    "\n",
    "[Python Project] Sentiment Analysis and Visualization of Stock News - YouTube https://www.youtube.com/watch?v=o-zM8onpQZY\n",
    "\n",
    "    -The sentiment analyzer that is used is an industry standard but it is not perfect. You will find titles that appear to have their sentiments mis-rated.\n",
    "\n",
    "    -This is NOT a stock picker nor is it intended to provide any information on a company's or stock prices' future, present or past performance. Any conclusions made from the information contained here-in are the sole responsibility of the user.\n",
    "    \n",
    "    -The information contained here-in is not exhaustive and should not be treated as such.\n",
    "    \n",
    "    -The accuracy of the information contained here-in is not verified. It could be wrong. \n",
    "    \n",
    "    -This application assumes a rudimentry understanding of the jupyter notebook environment. If you have absolutely no technical background, find someone who does and who is willing to help you. It may save you hours of frustration. \n",
    "\n",
    "To use this app.:\n",
    "\n",
    "    Run the cell titled \"RUN THIS CELL\" by selecting this cell (side bar will be blue) and pressing the Run button on the toolbar above (in the jupyter notebook environment there are other ways to run the code.) \n",
    "\n",
    "    Enter the stock symbol (not the company's name) you want to research. (It is not case sensitive.)\n",
    "    \n",
    "    After you have entered the stock symbol you will be prompted to see if you want to remove what are known as stopwords. Stopwords are words that are generally accepted as conveying no sentiment. If you choose to have them removed it might make the the code run more efficiently.  \n",
    "    \n",
    "    You should see a bar chart with the sentiment values for the most recent 100 articles with date listed on the x-axis and the nltk's VADER sentiment value on the y-axis. If no bar chart appears run through the all of the prompts and re-start the app.\n",
    "    \n",
    "    Next you will see a pie chart that is divided into \"positive,\" \"neutral,\" and \"negative\" sentiments and the accompanied statistics.\n",
    "    \n",
    "    Following this you will see a word cloud of the titles with postive sentiment. \n",
    "    \n",
    "    To find the posts where these words reside, type in the word in the prompt. These are case sentitive so if your search comes back empty, re-enter the word with changed capitization. For example, start with the first letter of the word capitalized with the rest of the letters in lower case. If that does not yield anything, try all lower case. Make sure your spelling is exactly what is represented in the word cloud. Enter only one word at the prompt.\n",
    "    \n",
    "    If you don't want to search on any of the key words, just press \"enter\" when asked what word to search on.\n",
    "    \n",
    "    Next you will see the word cloud for the neutral sentiment titles. Follow the same procedure as above to search or move on.\n",
    "    \n",
    "    Next you will see the word cloud for the negative sentiment titles. You have the same opportunity to search titles that have negative sentiments.\n",
    "    \n",
    "    When the program is finished, the message \"All done ...\" will appear.\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Rev 1 incorporates simple statistics and sorting based on nltk sentiment ratings. si\n",
    "\n",
    "Rev 2 incorporates stopwords removal and then plots word clouds based on sentiment ratings.; adds pie chart; search titles\n",
    "using key word. si\n",
    "\n",
    "Rev 2.1 changes in the structure of the program. encorporates methods and main into one cell for easier execution.\n",
    "\n",
    "Rev 2.2 - makes the input fields more robust to aberrant inputs\n",
    "        - changes \"no\" to \"n\" in another key word search request\n",
    "        - incoporates the loading of the libraries into the main body for ease of execution\n",
    "        - adds instructions for running the application\n",
    "        \n",
    "Rev 2.3 - add capability to remove stopwords from the titles\n",
    "        - added histogram of sentiment values from the 100 articles (JI)\n",
    "        - added handling of empty article list for word cloud (if there are no articles, the word cloud is not made)\n",
    "        - it might be good to add a word count feature ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Do not run this needed to use with jupyter binder\n",
    "#!pip install urlib.request\n",
    "!pip install bs4\n",
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install numpy \n",
    "#!pip install time\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revision of Librarries of last working build\n",
    "Requirement already satisfied: bs4 in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1)\n",
    "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.9.1)\n",
    "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
    "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.5)\n",
    "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
    "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
    "Requirement already satisfied: tqdm in c:\\users\\pstri\\appdata\\roaming\\python\\python38\\site-packages (from nltk) (4.60.0)\n",
    "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
    "Requirement already satisfied: pandas in c:\\users\\pstri\\appdata\\roaming\\python\\python38\\site-packages (1.2.4)\n",
    "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\pstri\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (1.19.5)\n",
    "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
    "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
    "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
    "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.2.2)\n",
    "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
    "Requirement already satisfied: numpy>=1.11 in c:\\users\\pstri\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (1.19.5)\n",
    "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
    "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
    "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
    "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
    "Requirement already satisfied: numpy in c:\\users\\pstri\\appdata\\roaming\\python\\python38\\site-packages (1.19.5)\n",
    "Requirement already satisfied: wordcloud in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.1)\n",
    "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (7.2.0)\n",
    "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\pstri\\appdata\\roaming\\python\\python38\\site-packages (from wordcloud) (1.19.5)\n",
    "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.2.2)\n",
    "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
    "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
    "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
    "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
    "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finviz scraping class\n",
    "#TODO imports to init.py?\n",
    "\n",
    "class finvizStreamer():\n",
    "    #Declare class variables\n",
    "    #---------------------\n",
    "    #declare dictionary (key:value)  to store ticker and associated html tables\n",
    "    news_tables = {}\n",
    "    #declare list (array) to store arrays of parsed data\n",
    "    parsed_data = []\n",
    "    tickers = []\n",
    "    #---------------------\n",
    "    #constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.url = 'https://www.finviz.com/quote.ashx?t='\n",
    "        #self.headers = {'Content-Type': 'application/json'}\n",
    " \n",
    "    def scrape_finziz(self):\n",
    "        from urllib.request import urlopen, Request\n",
    "        import json\n",
    "        from bs4 import BeautifulSoup\n",
    "        import pandas as pd\n",
    "\n",
    "        symbol = input('What is the symbol of the stock? (Please enter only one.)')\n",
    "\n",
    "        finviz_url = 'https://www.finviz.com/quote.ashx?t='\n",
    "        \n",
    "        tickers = [symbol]\n",
    "\n",
    "        for ticker in tickers:\n",
    "            url = self.url + ticker\n",
    "\n",
    "            req = Request(url = url, headers = {'user-agent': 'my-app'})\n",
    "            print (\"request url:\" +url)\n",
    "            response = urlopen(req)\n",
    "\n",
    "            html = BeautifulSoup(response, 'html')\n",
    "            news_table = html.find(id = 'news-table')\n",
    "            self.news_tables[ticker] = news_table\n",
    "    \n",
    "\n",
    "        for ticker, news_table in self.news_tables.items():\n",
    "            for row in news_table.findAll('tr'):\n",
    "\n",
    "                title = row.a.get_text()\n",
    "                link = row.a.get('href') #captures the link\n",
    "                date_data = row.td.text.split(' ')\n",
    "\n",
    "                if len(date_data) == 1: # if there is both a date and time it parses them into two columns\n",
    "                    time = date_data[0]\n",
    "                else: \n",
    "                    date = date_data[0] \n",
    "                    time = date_data[1]\n",
    "                self.parsed_data.append([ticker, date, time, title, link])\n",
    "        \n",
    "        df = pd.DataFrame(self.parsed_data, columns = ['ticker', 'date', 'time', 'title', 'link'])  \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analytics class\n",
    "\n",
    "class sentimentAnalytics():\n",
    "    #Declare class variables\n",
    "    #---------------------\n",
    "    \n",
    "\n",
    "    #---------------------\n",
    "    #constructor\n",
    "    #def __init__(self):\n",
    "        #This could have been done in the main cell but evenually this class will be moved into a pip installable\n",
    "        # package thus hiding the complexities from the non python user\n",
    "        \n",
    "        #TODO make these downloads definitions parameters?\n",
    "        \n",
    "    \n",
    "    def main(self, df, tickers):\n",
    "        from bs4 import BeautifulSoup\n",
    "        import nltk\n",
    "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        import time\n",
    "        from nltk.corpus import stopwords\n",
    "        \n",
    "        nltk.download('vader_lexicon')\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "        from nltk.corpus import stopwords\n",
    "        yes_answer = ['yes','YES','Yes','y','Y']\n",
    "        no_answer = ['no', 'NO', 'No', 'n', 'N'] \n",
    "        def remove_stopwords(): #provides a comprehensive list of stopwords; returns 'stopWords'\n",
    "\n",
    "            #440\n",
    "            from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "            from nltk.corpus import stopwords\n",
    "\n",
    "            #450\n",
    "            stopWords = set(stopwords.words('english'))\n",
    "\n",
    "            #print(len(stopWords))\n",
    "\n",
    "            #470 creates a list of new stopwords and then adds them to the set provided by nltk\n",
    "            # Note: it is case sensitive\n",
    "\n",
    "            newStopWords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "            newStopWords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "            newStopWords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "            newStopWords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "            newStopWords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "            newStopWords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "            newStopWords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "            newStopWords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "            newStopWords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "            newStopWords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "            newStopWords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "            newStopWords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "            newStopWords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "            newStopWords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "            newStopWords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "            newStopWords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "            newStopWords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "            newStopWords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "            newStopWords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "            newStopWords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "            newStopWords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "            newStopWords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "            newStopWords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "            newStopWords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "            newStopWords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "            newStopWords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "            newStopWords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "            newStopWords += ['nevertheless', 'next', 'nine', 'nobody', 'none'] #removed 'no'\n",
    "            newStopWords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "            newStopWords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "            newStopWords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "            newStopWords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "            newStopWords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "            newStopWords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "            newStopWords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "            newStopWords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "            newStopWords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "            newStopWords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "            newStopWords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "            newStopWords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "            newStopWords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "            newStopWords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "            newStopWords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "            newStopWords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "            newStopWords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "            newStopWords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "            newStopWords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "            newStopWords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "            newStopWords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "            newStopWords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "            newStopWords += ['yours', 'yourself', 'yourselves'] #provided by Codecademy??\n",
    "\n",
    "            # additional stopwords:\n",
    "            newStopWords += ['[Screenshot]', '[screenshot]', 'Screenshot', '[Screenshot]Great', '[SCREENSHOT]', 'screenshot', \n",
    "                         'The', 'the', 'SMART', 'yah', 'got', 'nutty', 'moving', 'weeks', 'Got', 'So', 'today', 'Been', 'or',\n",
    "                            \"n't\"]\n",
    "\n",
    "            newStopWords += ['I', 'it', 'It'] # pronouns\n",
    "\n",
    "            newStopWords += ['AMD', 'NVDA','NVDA', 'TSLA', 'GOOG', 'BA', 'FB', 'GOOGL', 'INTC', 'intel', 'Intel', 'CSCO', 'MU', \n",
    "                         'SMH', 'TSM','AAPL', 'TSLA', 'CSCO', 'POETF', 'PHOTONICS', 'DD', 'ARWR', 'T', 'INFI', 'AMC', 'ARK',\n",
    "                        'GME', 'NIO', 'QS', 'ADBE', 'MSFT'] # Stock symbols or names\n",
    "\n",
    "            newStopWords += ['Readytogo123', 'Maddog68','Stocktwits'] # nouns\n",
    "\n",
    "            newStopWords += ['.', '?', '!', ';', ',', \"'\"] # punctuation\n",
    "\n",
    "            newStopWords += ['&', '#', '%', '$', '@'] # symbols\n",
    "\n",
    "            newStopWords += ['41.75', '530.05', '39', 'Two', 'two',] # numbers\n",
    "\n",
    "            #adds them to the stopWords list provided by nltk\n",
    "            for i in newStopWords:\n",
    "                stopWords.add(i) #stopWords is defined as a \"set\" in #450 when inputed as english words from nltk;\n",
    "                # sets cannot be ordered so it must be converted back to a list to be ordered or alphabetized. A set has no duplicate elements.\n",
    "\n",
    "            #print(len(stopWords))\n",
    "            #print(stopWords)\n",
    "\n",
    "            #converts the set to a list\n",
    "            stopWords_list = list(stopWords)\n",
    "\n",
    "            #sorts the stopword list\n",
    "            stopWords_list.sort(key = lambda k : k.lower())\n",
    "            #print(stopWords_list)\n",
    "\n",
    "\n",
    "            #480 This removes words from the list of stopwords and writes list to csv file\n",
    "            # https://stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python#:~:text=one%20more%20easy%20way%20to%20remove%20words%20from,%3D%20words%20-%20stopwords%20final_list%20%3D%20list%20%28final_list%29\n",
    "            #new_words = list(filter(lambda w: w not in stop_words, initial_words))\n",
    "\n",
    "            WordsToBeRem = ['no'] #words to be removed from the stopword_list\n",
    "            stopWords = list(filter(lambda w: w not in WordsToBeRem, stopWords_list)) #stopWords_list has been sorted in #470\n",
    "\n",
    "            #converts the stopword list to a df and then outputs the df to a csv file\n",
    "            df_stopwords = pd.DataFrame(stopWords, columns = ['stopwords'])\n",
    "            df_stopwords.to_csv('stopwords.csv', index = False) #writes the csv file\n",
    "\n",
    "            return stopWords\n",
    "\n",
    "        def remove(df, stopWords): #returns a df where the stopwords are removed\n",
    "\n",
    "            dfScrubbed = df.copy() #This is a deep copy. df.copy(deep = True); deep = True is default\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            while i < len(df):\n",
    "\n",
    "                data = df.iloc[i,1] #column #1 holds the titles of the posts\n",
    "                words = word_tokenize(data) #the title is separated into individual words (tokenized)\n",
    "                wordsFiltered = []\n",
    "\n",
    "                for w in words:\n",
    "                    if w not in stopWords:\n",
    "                        wordsFiltered.append(w)\n",
    "\n",
    "                joinedWordsFiltered = ' '.join(wordsFiltered) #combines the individual words into one string\n",
    "\n",
    "                dfScrubbed.iloc[i,1] = joinedWordsFiltered # replaces the recorded in dfAPIScrubbed with the stopWords removed\n",
    "                #from the 'body'\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            #print(wordsFiltered)\n",
    "\n",
    "            #print(dfScrubbed.head())\n",
    "\n",
    "            return(dfScrubbed)\n",
    "\n",
    "        def wc(df): #creates the word cloud\n",
    "            #from wordcloud import WordCloud, STOPWORDS \n",
    "            from wordcloud import WordCloud\n",
    "            import matplotlib.pyplot as plt \n",
    "            import pandas as pd \n",
    "\n",
    "            stopwords = set(stopWords) \n",
    "            words = ''\n",
    "            for review in df.title:\n",
    "                tokens = str(review).split()\n",
    "                tokens = [i.lower() for i in tokens]\n",
    "\n",
    "                words += ' '.join(tokens) + ' '\n",
    "\n",
    "            wordcloud = WordCloud(width = 800, height = 800, \n",
    "                        background_color ='white', \n",
    "                        stopwords = stopwords, \n",
    "                        min_font_size = 10).generate(words) \n",
    "\n",
    "            # plot the WordCloud image                        \n",
    "            plt.figure(figsize = (8, 8), facecolor = None) \n",
    "            plt.imshow(wordcloud) \n",
    "            plt.axis(\"off\") \n",
    "            plt.tight_layout(pad = 0) \n",
    "\n",
    "            plt.show() \n",
    "\n",
    "        def kw(df,keyword): # searches a string for key words; if found will print out the date and title\n",
    "            i = 0\n",
    "            while i < len(df):\n",
    "\n",
    "                data = df.iloc[i,1] #column #1 holds the titles of the posts\n",
    "                a_bool = keyword.lower() in data.lower()\n",
    "\n",
    "                if a_bool == True:\n",
    "                    print(df.iloc[i,0], df.iloc[i,1])\n",
    "                    print(df.iloc[i,2], '\\n')\n",
    "\n",
    "                i += 1\n",
    "\n",
    "        def search_repeat(df): # provides opportunity to do multiple searches on key words. returns only the appropriate yes or no response.\n",
    "            key_word = input('What is the key word you want to search? [press \"enter\" for none]')\n",
    "            if key_word: #True if anything is entered in the key_word inputstatement; False if only enter is pressed\n",
    "                kw(df, key_word)\n",
    "                answer = input('Do you want to do another search? [Enter either a \"y\" or \"n.\"]')\n",
    "            else:\n",
    "                answer = 'no' #if there is not key word that is entered it sets answer to no. - assumes if there is no key word there is no desire to do another search.\n",
    "            while answer not in yes_answer and answer not in no_answer: # Restricts answer to be either in the yes or no list by continuous looping on it unit input matches either list\n",
    "                answer = error() # prompts for the correct yes or no response. The correct responses are in the yes_answer list and no_answer list.\n",
    "            return answer\n",
    "\n",
    "        def error(): # provides user the opportunity to correct the user's input\n",
    "            correction = input('Your input needs to be either a \"y\" or a \"n\". Would you like to do another search?')\n",
    "            return correction\n",
    "\n",
    "        def error1(): # provides user the opportunity to correct the user's input\n",
    "            correction = input('Your input needs to be either a \"y\" or a \"n\". Would you like to remove the stopwords from the titles?')\n",
    "            return correction\n",
    "\n",
    "        def stopwords_yes_no(): # provides opportunity to removes stopwords from the titles. returns only the appropriate yes or no response.\n",
    "            yes_no = input('Do you want to remove the stopwords from the titles? [press \"enter\" for no]')\n",
    "            if yes_no in yes_answer:\n",
    "                answer = 'yes'\n",
    "            else:\n",
    "                answer = 'no' #if there is not key word that is entered it sets answer to no. - assumes if there is no key word there is no desire to do another search.\n",
    "            while answer not in yes_answer and answer not in no_answer: # Restricts answer to be either in the yes or no list by continuous looping on it unit input matches either list\n",
    "                answer = error1() # prompts for the correct yes or no response. The correct responses are in the yes_answer list and no_answer list.\n",
    "            return answer\n",
    "    \n",
    "    #### REMOVES STOPWORDS\n",
    "        sw_answer = stopwords_yes_no() #returns either a 'yes' or 'no' from the user's input\n",
    "        if sw_answer == 'yes':\n",
    "            stopWords = remove_stopwords() #provides a comprehensive list of stopwords; returns 'stopWords'\n",
    "            dfScrubbed = remove(df, stopWords) #returns a df where the stopwords are removed\n",
    "            print('\\nThe stopwords will be removed. \\n')\n",
    "\n",
    "        #### PERFORMS THE VADER SENTIMENT ANALYSIS.\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "        f = lambda title: vader.polarity_scores(title)['compound']\n",
    "\n",
    "        if sw_answer == 'yes': #uses dfScrubbed to generate sentiment values if uses said yes\n",
    "            df['compound'] = dfScrubbed['title'].apply(f) # uses the scrubbed title data to generate the sentiment score and places the result back into the non-scrubbed df\n",
    "        else:\n",
    "            df['compound'] = df['title'].apply(f) # does not uses the scrubbed titles to produce the sentiment values\n",
    "\n",
    "\n",
    "        #print(df.head()) # commented out by si\n",
    "        #print(len(df)) # commented out by si\n",
    "\n",
    "        #### PLOTS AVERAGE SENTIMENT VALUES AS A FUNCTION OF DATES\n",
    "        df ['date'] = pd.to_datetime(df.date).dt.date        \n",
    "\n",
    "        plt.figure(figsize = (10 ,8))\n",
    "\n",
    "        mean_df = df.groupby(['ticker', 'date']).mean()\n",
    "        mean_df = mean_df.unstack()\n",
    "        mean_df = mean_df.xs('compound', axis = 'columns').transpose()\n",
    "\n",
    "        mean_df.plot(kind = 'bar')\n",
    "\n",
    "        print(tickers)\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(20,10) #adjusts bar chart's size\n",
    "        plt.show()\n",
    "        print('Here is a bar chart of the average sentiment values by date!')\n",
    "\n",
    "        import time\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        pause = input('Press enter to move on. Coming up is the histogram of sentiment values!') #pauses after the sentiment by date histogram\n",
    "\n",
    "        #### PROVIDES DATE RANGES AND SIMPLE STATISTICS ON THE SENTIMENT OF THE TITLES\n",
    "\n",
    "        # provides date ranges for the last 100 articles; added by si\n",
    "        print('Date Range of the 100 most recent articles: ') #added by si\n",
    "        print('Most Recent Article Date: ', df.iloc[0,1]) #added by si\n",
    "        #print('Oldest Article Date: ', df.iloc[99,1], '\\n') #added by si\n",
    "        oldest = len(df) - 1\n",
    "        print('Oldest Article Date: ', df.iloc[oldest,1], '\\n') #added by si\n",
    "\n",
    "\n",
    "        # provides basic sentiment statistics; added by si\n",
    "        i = 0 # set starting index number to 0\n",
    "        pos_counter = 0 # sets starting positive counter to 0\n",
    "        neu_counter = 0\n",
    "        neg_counter = 0\n",
    "\n",
    "        dfpos = pd.DataFrame(columns = ['date', 'title', 'link']) #initializes df where positive titles are stored\n",
    "        dfneu = pd.DataFrame(columns = ['date', 'title', 'link'])\n",
    "        dfneg = pd.DataFrame(columns = ['date', 'title', 'link'])\n",
    "\n",
    "        # for the sentiment histogram\n",
    "        sent_hist = []\n",
    "\n",
    "        # Separate the sentiment values into pos, neu, and neg\n",
    "        while i < len(df):\n",
    "            sent_hist.append(df.iloc[i,5]) # added for the sentiment histogram\n",
    "            if df.iloc[i,5] > 0.0:\n",
    "                pos_counter += 1\n",
    "                dfpos = dfpos.append(dict(zip(dfpos.columns,[df.iloc[i,1], df.iloc[i,3], df.iloc[i,4]])), ignore_index=True) #fill dfpos df with date/time, title & link\n",
    "\n",
    "            elif df.iloc[i,5] == 0.0:\n",
    "                    neu_counter += 1\n",
    "                    dfneu = dfneu.append(dict(zip(dfneu.columns,[df.iloc[i,1], df.iloc[i,3], df.iloc[i,4]])), ignore_index=True)\n",
    "\n",
    "            elif df.iloc[i,5] < 0.0:\n",
    "                    neg_counter += 1\n",
    "                    dfneg = dfneg.append(dict(zip(dfneg.columns,[df.iloc[i,1], df.iloc[i,3], df.iloc[i,4]])), ignore_index=True)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #### SENTIMENT HISTOGRAM by sentiment value\n",
    "        sent_hist = np.asarray(sent_hist)\n",
    "        plt.figure()\n",
    "        #plt.hist(sent_hist, bins=20, range=[-1.0, 1.0])\n",
    "        plt.hist(sent_hist, bins=[-1.0,-0.95,-0.85,-0.75, -0.65, -0.55, -0.45, -0.45, -0.35, -0.25, -0.15,\n",
    "                                  -0.05, 0.05, 0.10, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, \n",
    "                                  1.0], range = [-1.0, 1.0]) \n",
    "        plt.title('Histogram of Sentiment Values')\n",
    "        plt.xlabel('Sentiment Value')\n",
    "        plt.ylabel('Number of articles')\n",
    "        plt.grid()\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(16,5) #adjusts the histogram's size\n",
    "        plt.show()\n",
    "\n",
    "        print('Here is a histogram of the sentiment values!')\n",
    "        print('NOTE: The 0.0 bar contains both neutral and no comment sentiments.')\n",
    "\n",
    "        time.sleep(1)\n",
    "        pause = input('Press enter to move on. Coming up is the sentiment pie chart!\\n') #pauses after the sentiment by value\n",
    "\n",
    "        #### PIE CHART\n",
    "\n",
    "        # Data to plot\n",
    "        labels = 'Positive', 'Neutral', 'Negative'\n",
    "        sizes = [pos_counter, neu_counter, neg_counter]\n",
    "        colors = ['lightblue', 'orange', 'pink']\n",
    "        explode = (0.1, 0, 0)  # explode 1st slice\n",
    "\n",
    "        # Plot\n",
    "        #plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "        plt.axis('equal')\n",
    "\n",
    "        print(tickers)\n",
    "        print('The percent of articles with Positive, Neutral and Negative sentiment.')\n",
    "        \n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(10,10) #adjusts the pie chart's size\n",
    "        plt.show()\n",
    "\n",
    "        time.sleep(5)\n",
    "        \n",
    "        pause = input('Press enter to move on. Coming up is the postive sentiment word cloud!\\n') #pauses after the pie chart\n",
    "\n",
    "        #### produces the word clouds; added by si\n",
    "        print('\\n*****************')\n",
    "        print('POSITIVE SENTIMENT: ')\n",
    "        print('The number of positive sentiment numbers is: ', pos_counter)\n",
    "        print('The percent of postive sentiment numbers is: ', pos_counter/len(df) * 100,'% \\n')\n",
    "\n",
    "        time.sleep(1)\n",
    "        if pos_counter != 0:\n",
    "            wc(remove(dfpos,remove_stopwords())) #creates the word cloud\n",
    "        else:\n",
    "            print('There are no positive articles.')\n",
    "\n",
    "        #### Title searches on key words for postive ratings\n",
    "\n",
    "        time.sleep(1)\n",
    "        repeat = 'yes' #initializes repeat to 'yes'; the user can/will change this in the search_repeat() method\n",
    "\n",
    "        while repeat in yes_answer:\n",
    "            repeat = search_repeat(dfpos)\n",
    "\n",
    "        print('Moving on to the neutral sentiment word cloud ...')\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        print('\\n*****************')\n",
    "        print('NEUTRAL SENTIMENT:')\n",
    "        print('The number of neutral sentiment numbers is: ', neu_counter)\n",
    "        print('The percent of neutral sentiment numbers is: ', neu_counter/len(df) * 100,'% \\n')\n",
    "\n",
    "        if neu_counter != 0:\n",
    "            wc(remove(dfneu,remove_stopwords())) #creates the word cloud\n",
    "        else:\n",
    "            print('There are no neutral articles.')\n",
    "\n",
    "        #### Title searches on key words for neutral ratings\n",
    "        time.sleep(1)\n",
    "        repeat = 'yes'\n",
    "        while repeat in yes_answer:\n",
    "            repeat = search_repeat(dfneu)\n",
    "\n",
    "        print('Moving on to the negative sentiment word clould ...')\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        print('\\n*****************')\n",
    "        print('NEGATIVE SENTIMENT: ')\n",
    "        print('The number of negative sentiment numbers is: ', neg_counter)\n",
    "        print('The percent of negativetive sentiment numbers is: ', neg_counter/len(df) * 100,'% \\n')\n",
    "\n",
    "        if neg_counter != 0:\n",
    "            wc(remove(dfneg,remove_stopwords())) #creates the word cloud\n",
    "        else:\n",
    "            print('There are no negative articles.')\n",
    "\n",
    "        #### Title searches on key words for negative ratings\n",
    "        time.sleep(1)\n",
    "        repeat = 'yes'\n",
    "        while repeat in yes_answer:\n",
    "            repeat = search_repeat(dfneg)\n",
    "\n",
    "        print('All done ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'finVizFetchPkg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fd3a159df8cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mfinVizFetchPkg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinVizScaper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msentimentAnalysisPkg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasicSentimentAnalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# instantiate class to scrape finviz data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#finvizdf = finvizStreamer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfinvizdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinVizFetchPkg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinVizScaper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinvizStreamer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'finVizFetchPkg'"
     ]
    }
   ],
   "source": [
    "import finVizFetchPkg.finVizScaper\n",
    "import sentimentAnalysisPkg.basicSentimentAnalysis\n",
    "# instantiate class to scrape finviz data\n",
    "#finvizdf = finvizStreamer()\n",
    "finvizdf = finVizFetchPkg.finVizScaper.finvizStreamer()\n",
    "df = finvizdf.scrape_finziz()\n",
    "#------------------------\n",
    "#instantiate class to perform sentiment analysis\n",
    "#sentiment = sentimentAnalytics()\n",
    "sentiment = sentimentAnalysisPkg.basicSentimentAnalysis.sentimentAnalytics()\n",
    "sentiment.main(df,finvizdf.tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "altgraph==0.17\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.10.0\n",
      "anyio @ file:///C:/ci/anyio_1617783481233/work/dist\n",
      "argon2-cffi @ file:///C:/ci/argon2-cffi_1613037959010/work\n",
      "async-generator @ file:///home/ktietz/src/ci/async_generator_1611927993394/work\n",
      "attrs @ file:///tmp/build/80754af9/attrs_1604765588209/work\n",
      "Babel @ file:///tmp/build/80754af9/babel_1607110387436/work\n",
      "backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\n",
      "beautifulsoup4==4.9.3\n",
      "bleach @ file:///tmp/build/80754af9/bleach_1612211392645/work\n",
      "brotlipy==0.7.0\n",
      "bs4==0.0.1\n",
      "build==0.5.1\n",
      "certifi==2020.12.5\n",
      "cffi @ file:///C:/ci/cffi_1605538112425/work\n",
      "chardet @ file:///C:/ci/chardet_1605303225733/work\n",
      "click==7.1.2\n",
      "clyent==1.2.2\n",
      "colorama @ file:///tmp/build/80754af9/colorama_1607707115595/work\n",
      "conda==4.10.1\n",
      "conda-package-handling @ file:///C:/ci/conda-package-handling_1603003327818/work\n",
      "cryptography @ file:///C:/ci/cryptography_1605526388508/work\n",
      "cycler==0.10.0\n",
      "decorator @ file:///tmp/build/80754af9/decorator_1617916966915/work\n",
      "defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\n",
      "docutils==0.17.1\n",
      "entrypoints==0.3\n",
      "finVizFetchPkg==0.0.1\n",
      "future==0.18.2\n",
      "idna @ file:///tmp/build/80754af9/idna_1593446292537/work\n",
      "importlib-metadata @ file:///C:/ci/importlib-metadata_1617877484576/work\n",
      "ipykernel @ file:///C:/ci/ipykernel_1596190155316/work/dist/ipykernel-5.3.4-py3-none-any.whl\n",
      "ipython @ file:///C:/ci/ipython_1617121002983/work\n",
      "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\n",
      "jedi==0.17.0\n",
      "Jinja2 @ file:///tmp/build/80754af9/jinja2_1612213139570/work\n",
      "joblib==1.0.1\n",
      "json5==0.9.5\n",
      "jsonschema @ file:///tmp/build/80754af9/jsonschema_1602607155483/work\n",
      "jupyter-client @ file:///tmp/build/80754af9/jupyter_client_1616770841739/work\n",
      "jupyter-core @ file:///C:/ci/jupyter_core_1612213356021/work\n",
      "jupyter-packaging @ file:///tmp/build/80754af9/jupyter-packaging_1613502826984/work\n",
      "jupyter-server @ file:///C:/ci/jupyter_server_1616084298403/work\n",
      "jupyterlab @ file:///tmp/build/80754af9/jupyterlab_1619133235951/work\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\n",
      "jupyterlab-server @ file:///tmp/build/80754af9/jupyterlab_server_1617134334258/work\n",
      "keyring==23.0.1\n",
      "kiwisolver==1.3.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.4.1\n",
      "menuinst==1.4.16\n",
      "mistune==0.8.4\n",
      "navigator-updater==0.2.1\n",
      "nbclassic @ file:///tmp/build/80754af9/nbclassic_1616085367084/work\n",
      "nbclient @ file:///tmp/build/80754af9/nbclient_1614364831625/work\n",
      "nbconvert @ file:///C:/ci/nbconvert_1601914925608/work\n",
      "nbformat @ file:///tmp/build/80754af9/nbformat_1617383369282/work\n",
      "nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1613680548246/work\n",
      "nltk==3.6.2\n",
      "notebook @ file:///C:/ci/notebook_1616443715883/work\n",
      "numpy==1.20.2\n",
      "olefile==0.46\n",
      "packaging @ file:///tmp/build/80754af9/packaging_1611952188834/work\n",
      "pandas==1.2.4\n",
      "pandocfilters @ file:///C:/ci/pandocfilters_1605102497129/work\n",
      "parso @ file:///tmp/build/80754af9/parso_1617223946239/work\n",
      "pefile==2019.4.18\n",
      "pep517==0.10.0\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\n",
      "Pillow @ file:///C:/ci/pillow_1617386341487/work\n",
      "pkginfo==1.7.0\n",
      "prometheus-client @ file:///tmp/build/80754af9/prometheus_client_1618088486455/work\n",
      "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1616415428029/work\n",
      "psutil @ file:///C:/ci/psutil_1612298324802/work\n",
      "pycosat==0.6.3\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\n",
      "Pygments @ file:///tmp/build/80754af9/pygments_1615143339740/work\n",
      "pyinstaller==4.3\n",
      "pyinstaller-hooks-contrib==2021.1\n",
      "pyOpenSSL @ file:///tmp/build/80754af9/pyopenssl_1605545627475/work\n",
      "pyparsing @ file:///home/linux1/recipes/ci/pyparsing_1610983426697/work\n",
      "pyrsistent @ file:///C:/ci/pyrsistent_1600141795814/work\n",
      "PySocks @ file:///C:/ci/pysocks_1605287845585/work\n",
      "python-dateutil @ file:///home/ktietz/src/ci/python-dateutil_1611928101742/work\n",
      "pytz @ file:///tmp/build/80754af9/pytz_1612215392582/work\n",
      "pywin32==227\n",
      "pywin32-ctypes==0.2.0\n",
      "pywinpty==0.5.7\n",
      "PyYAML==5.4.1\n",
      "pyzmq==20.0.0\n",
      "QtPy==1.9.0\n",
      "readme-renderer==29.0\n",
      "regex==2021.4.4\n",
      "requests @ file:///tmp/build/80754af9/requests_1592841827918/work\n",
      "requests-toolbelt==0.9.1\n",
      "rfc3986==1.5.0\n",
      "ruamel-yaml==0.15.87\n",
      "Send2Trash @ file:///tmp/build/80754af9/send2trash_1607525499227/work\n",
      "sentimentAnalysisPkg==0.0.2\n",
      "sip==4.19.13\n",
      "six @ file:///C:/ci/six_1605187374963/work\n",
      "sniffio @ file:///C:/ci/sniffio_1614030707456/work\n",
      "soupsieve==2.2.1\n",
      "terminado==0.9.4\n",
      "testpath @ file:///home/ktietz/src/ci/testpath_1611930608132/work\n",
      "toml==0.10.2\n",
      "tornado @ file:///C:/ci/tornado_1606942392901/work\n",
      "tqdm @ file:///tmp/build/80754af9/tqdm_1605303662894/work\n",
      "traitlets @ file:///home/ktietz/src/ci/traitlets_1611929699868/work\n",
      "twine==3.4.1\n",
      "urllib3 @ file:///tmp/build/80754af9/urllib3_1603305693037/work\n",
      "wcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work\n",
      "webencodings==0.5.1\n",
      "win-inet-pton @ file:///C:/ci/win_inet_pton_1605306167264/work\n",
      "wincertstore==0.2\n",
      "wordcloud==1.8.1\n",
      "xmltodict==0.12.0\n",
      "zipp @ file:///tmp/build/80754af9/zipp_1615904174917/work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -heel (c:\\programdata\\miniconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
